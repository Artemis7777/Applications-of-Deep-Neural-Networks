{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team: Kitty\n",
    "# Members: Minjie Yang (Leader), Yu Ren, Peter Xie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n",
    "\n",
    "The original author of this part is Professor Jeffrey Heaton.\n",
    "\n",
    "Remark: to_xy has been tailored to the Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    # Regression\n",
    "    return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function in 'datasets_toy1.ipynb' to calculate weight with the toy's metal, shape, height, length and weight \n",
    "import math\n",
    "def calculate_weight(metal,shape,h,l,w):\n",
    "    metal_name = ['gold','silver','bronze','tin','platinum']\n",
    "    metal_density = [19.32,10.49, 9.29,7.31, 21.09]\n",
    "    shape_name = ['sphere','box','cylinder']\n",
    "    \n",
    "    metal = metal_name.index(metal)\n",
    "    shape = shape_name.index(shape)\n",
    "    \n",
    "    if shape==0:\n",
    "        # sphere\n",
    "        vol = (4.0/3.0)  * math.pi * ((l/2.0)**3)\n",
    "    elif shape==1:\n",
    "        # box\n",
    "        vol = l * w * h\n",
    "    elif shape==2:\n",
    "        # cylinder\n",
    "        vol = math.pi * ((w/2.0)**2.0) * h\n",
    "        \n",
    "    weight = vol * metal_density[metal]\n",
    "        \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Project Code\n",
    "\n",
    "The author of the rest (main part) is Minjie Yang.\n",
    "\n",
    "**Description:**\n",
    "\n",
    "* Inspired from the midterm exam, **one attempt is to encode metal into its corresponding density** (from 'datasets_toy1.ipynb') in order to give it both a numeric value and an order. Doing this could reach a score around 290-300 on Kaggle.\n",
    "\n",
    "* **Another attempt is to split up the dataset according to the cost column** for the following motivation. Doing this could reach a score around 200-210 on Kaggle.\n",
    "\n",
    "The toy's **weight** can be evaluated in the following two ways:\n",
    "\n",
    "**1.weight = cost/metal_cost**\n",
    "\n",
    "**2.weight = density\\*volume** (can be realized by the helpful function calculate_weight)\n",
    "\n",
    "After analyzing the dataset and running some experiments, we find out that the first way has a high accuracy in predicting **weight** (RMSE on the holdout set is around 5), while the second way has a low accuracy (around 400, and this result is about the same among all three shapes). Unfortunately, there is a lot of missing value in the **cost** column. Simply encoding the missing data of the **cost** column into its median will lead to a bad performance, because it will destroy the nice feature of it. So in order to good full use of **cost**, we separate the dataset into two parts:\n",
    "\n",
    "**If the cost value is complete: com_train and com_test**\n",
    "\n",
    "**If the cost value is missing: mis_train and mis_test**\n",
    "\n",
    "On the **com_train** and **com_test**, we add an approximation column predicting the **weight** by the first way to enhance the performance. On the **mis_train** and **mis_test**, we add an approximation column predicting the **weight** by the second way.\n",
    "\n",
    "Eventually, we choose the two predictions on **com_test** and **mis_test** from the two models with the best holdout score respectively. Then we concatenate them together into one final prediction **df_submit**.\n",
    "\n",
    "So the code of the model will be organized by:\n",
    "\n",
    "**0.com_train, com_test, mis_train and mis_test preparing**\n",
    "\n",
    "**1.com_train and com_test feature encoding > com_train training > com_test predicting : df_com**\n",
    "\n",
    "**2.mis_train and mis_test feature encoding > mis_train training > mis_test predicting : df_mis**\n",
    "\n",
    "**3.concatenate df_com and df_mis together into df_submit**\n",
    "\n",
    "If want to try some changes on the neural network, just run the training and predicting part (with respect to 1 or 2), then run 3 to update **df_submit**.\n",
    "\n",
    "The final two submissions are chosen by prediction with epoch=60 and epoch=200 to prevent overfitting.\n",
    "\n",
    "Remark: \n",
    "\n",
    "Some other hyper parameter choosing: batch_size = 200, neural network size = (20 40 80), applying zscore before training, etc. What's more: led_vol, motor_vol, gear_vol, volume_parts are dropped because of nearly missing all the value and having low relevance to **weight**. Inverse pyramid structure doesn't help, wide/deep network doesn't help much, dropout layer doesn't help much, early stopping doesn't help much. The score on holdout set is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages and dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = \"./all/\"\n",
    "    \n",
    "filename_train = os.path.join(path,\"train.csv\")\n",
    "filename_test = os.path.join(path,\"test.csv\")\n",
    "df_train = pd.read_csv(filename_train,na_values=['NA','?'])\n",
    "df_test = pd.read_csv(filename_test,na_values=['NA','?'])\n",
    "\n",
    "# preparing dataset\n",
    "# 'id' is irrelevant to the prediction\n",
    "df_train.drop('id',axis=1,inplace=True)\n",
    "# 'led_vol','motor_vol','gear_vol','volume_parts' are hardly irrelevant to the prediction\n",
    "df_train.drop(['led_vol','motor_vol','gear_vol','volume_parts'],axis=1,inplace=True)\n",
    "df_test.drop(['led_vol','motor_vol','gear_vol','volume_parts'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. com_train, com_test, mis_train and mis_test preparing\n",
    "mis_train = df_train[df_train['cost'].isna() == True] # missing data\n",
    "mis_test = df_test[df_test['cost'].isna() == True]\n",
    "com_train = df_train[df_train['cost'].isna() == False] # complete data\n",
    "com_test = df_test[df_test['cost'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. com_train and com_test feature encoding\n",
    "# saving 'id' to match the prediction\n",
    "id_com = com_test['id']\n",
    "com_test.drop('id',axis=1,inplace=True)\n",
    "\n",
    "# encode 'metal' into its corresponding density\n",
    "com_train['metal'].replace(['gold','silver','bronze','tin','platinum'],[19.32,10.49, 9.29,7.31, 21.09],inplace=True)\n",
    "com_test['metal'].replace(['gold','silver','bronze','tin','platinum'],[19.32,10.49, 9.29,7.31, 21.09],inplace=True)\n",
    "\n",
    "# approximation 1: weight = cost/metal_cost\n",
    "com_train['approx'] = com_train.apply(lambda x: x['cost']/x['metal_cost'], axis=1)\n",
    "com_test['approx'] = com_test.apply(lambda x: x['cost']/x['metal_cost'], axis=1)\n",
    "\n",
    "encode_numeric_zscore(com_train,['metal_cost','height','width','length','led','gears','motors','metal','cost','approx'])\n",
    "encode_numeric_zscore(com_test,['metal_cost','height','width','length','led','gears','motors','metal','cost','approx'])\n",
    "\n",
    "encode_text_dummy(com_train,'shape')\n",
    "encode_text_dummy(com_test,'shape')\n",
    "\n",
    "x,y = to_xy(com_train,'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com_train training\n",
    "batch_size = 200\n",
    "# Cross validation & Holdout & Trainning\n",
    "\n",
    "# Keep a 10% holdout\n",
    "x_main, x_holdout, y_main, y_holdout = train_test_split(    \n",
    "    x, y, test_size=0.10) \n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x_main):        \n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x_main[train]\n",
    "    y_train = y_main[train]\n",
    "    x_test = x_main[test]\n",
    "    y_test = y_main[test]\n",
    "    \n",
    "    model_com = Sequential()\n",
    "    model_com.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "    # model_com.add(Dropout(0.01))\n",
    "    model_com.add(Dense(40, activation='relu'))\n",
    "    model_com.add(Dense(80, activation='relu'))\n",
    "    model_com.add(Dense(1))\n",
    "    model_com.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    # model_com.fit(x_train,y_train,batch_size=batch_size,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    model_com.fit(x_train,y_train,batch_size=batch_size,validation_data=(x_test,y_test),verbose=0,epochs=60)\n",
    "    model_com.save(\"com%d.hdf5\" % fold)\n",
    "    \n",
    "    pred = model_com.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred) \n",
    "\n",
    "    # Measure accuracy\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print()\n",
    "print(\"Cross-validated score (RMSE): {}\".format(score))    \n",
    "    \n",
    "# Write the cross-validated prediction (from the last neural network)\n",
    "holdout_pred = model_com.predict(x_holdout)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "print(\"Holdout score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com_test predicting : df_com\n",
    "\n",
    "# # to choose the best model\n",
    "# x,y = to_xy(com_train,'weight')\n",
    "# from keras.models import load_model\n",
    "# model_com = load_model(\"com2.hdf5\") # change the number\n",
    "# # Keep a 10% holdout \n",
    "# x_main, x_holdout, y_main, y_holdout = train_test_split(x, y, test_size=0.10) \n",
    "# # Write the cross-validated prediction\n",
    "# holdout_pred = model_com.predict(x_holdout)\n",
    "# score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "# print(\"Holdout score (RMSE): {}\".format(score))\n",
    "\n",
    "# default is the last model\n",
    "x = com_test.as_matrix().astype(np.float32)\n",
    "\n",
    "pred = model_com.predict(x)\n",
    "\n",
    "df_com = pd.DataFrame(pred)\n",
    "df_com.insert(0,'id',list(id_com))\n",
    "df_com.columns = ['id','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.mis_train and mis_test feature encoding\n",
    "# 'cost' is missing, drop it\n",
    "# keep 'metal_cost' to add a protential feature\n",
    "mis_train.drop(['cost'],axis=1,inplace=True)\n",
    "mis_test.drop(['cost'],axis=1,inplace=True)\n",
    "\n",
    "# saving 'id' to match the prediction\n",
    "id_mis = mis_test['id']\n",
    "mis_test.drop('id',axis=1,inplace=True)\n",
    "\n",
    "# approximation 2: weight = density*volume(can be realized by the helpful function calculate_weight)\n",
    "mis_train['approx'] = mis_train.apply(lambda x: calculate_weight(x['metal'],x['shape'],x['height'],x['length'],x['width']), axis=1)\n",
    "mis_test['approx'] = mis_test.apply(lambda x: calculate_weight(x['metal'],x['shape'],x['height'],x['length'],x['width']), axis=1)\n",
    "\n",
    "# encode 'metal' into its corresponding density\n",
    "mis_train['metal'].replace(['gold','silver','bronze','tin','platinum'],[19.32,10.49, 9.29,7.31, 21.09],inplace=True)\n",
    "mis_test['metal'].replace(['gold','silver','bronze','tin','platinum'],[19.32,10.49, 9.29,7.31, 21.09],inplace=True)\n",
    "\n",
    "encode_numeric_zscore(mis_train,['metal','metal_cost','height','width','length','led','gears','motors','approx'])\n",
    "encode_numeric_zscore(mis_test,['metal','metal_cost','height','width','length','led','gears','motors','approx'])\n",
    "\n",
    "encode_text_dummy(mis_train,'shape')\n",
    "encode_text_dummy(mis_test,'shape')\n",
    "\n",
    "x,y = to_xy(mis_train,'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mis_train training\n",
    "batch_size = 200\n",
    "# Cross validation & Holdout & Trainning\n",
    "\n",
    "# Keep a 10% holdout\n",
    "x_main, x_holdout, y_main, y_holdout = train_test_split(    \n",
    "    x, y, test_size=0.10) \n",
    "\n",
    "# Cross-validate\n",
    "kf = KFold(5)\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x_main):        \n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x_main[train]\n",
    "    y_train = y_main[train]\n",
    "    x_test = x_main[test]\n",
    "    y_test = y_main[test]\n",
    "    \n",
    "    model_mis = Sequential()\n",
    "    model_mis.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
    "    # model_mis.add(Dropout(0.01))\n",
    "    model_mis.add(Dense(40, activation='relu'))\n",
    "    model_mis.add(Dense(80, activation='relu'))\n",
    "    model_mis.add(Dense(1))\n",
    "    model_mis.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    # model_mis.fit(x_train,y_train,batch_size=batch_size,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=1000)\n",
    "    model_mis.fit(x_train,y_train,batch_size=batch_size,validation_data=(x_test,y_test),verbose=0,epochs=60)\n",
    "    model_mis.save(\"mis%d.hdf5\" % fold)\n",
    "    \n",
    "    pred = model_mis.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred) \n",
    "\n",
    "    # Measure accuracy\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
    "print()\n",
    "print(\"Cross-validated score (RMSE): {}\".format(score))    \n",
    "    \n",
    "# Write the cross-validated prediction (from the last neural network)\n",
    "holdout_pred = model_mis.predict(x_holdout)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "print(\"Holdout score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mis_test predicting : df_mis\n",
    "\n",
    "# # to choose the best model\n",
    "# x,y = to_xy(mis_train,'weight')\n",
    "# from keras.models import load_model\n",
    "# model_mis = load_model(\"mis5.hdf5\") # change the number\n",
    "# # Keep a 10% holdout\n",
    "# x_main, x_holdout, y_main, y_holdout = train_test_split(x, y, test_size=0.10) \n",
    "# # Write the cross-validated prediction\n",
    "# holdout_pred = model_mis.predict(x_holdout)\n",
    "# score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "# print(\"Holdout score (RMSE): {}\".format(score))\n",
    "\n",
    "# default is the last model\n",
    "x = mis_test.as_matrix().astype(np.float32)\n",
    "\n",
    "pred = model_mis.predict(x)\n",
    "\n",
    "df_mis = pd.DataFrame(pred)\n",
    "df_mis.insert(0,'id',list(id_mis))\n",
    "df_mis.columns = ['id','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.concatenate df_com and df_mis together into df_submit\n",
    "df_submit = pd.concat([df_com,df_mis])\n",
    "df_submit.sort_values(by=\"id\",ascending= True,inplace=True) \n",
    "\n",
    "# Save csv\n",
    "df_submit.to_csv('kaggle.csv',index=False)\n",
    "df_submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
